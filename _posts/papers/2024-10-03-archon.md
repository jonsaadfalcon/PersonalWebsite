---
layout: paper
categories: papers
permalink: papers/archon
id: ares
title: "Archon: An Architecture Search Framework for Inference-Time Techniques"
authors: 
  - Jon Saad-Falcon
  - Adrian Lafuente Gamarra
  - Shlok Natarajan
  - Nahum Maru
  - Hristo Todorov
  - Etash Guha
  - E. Kelly Buchanan
  - Mayee Chen
  - Neel Guha
  - Christopher Ré
  - Azalia Mirhoseini
venue: 
year: 
url: /papers/archon
pdf: https://arxiv.org/abs/2409.15254
code: https://github.com/ScalingIntelligence/Archon
type: 
figure: /images/papers/archon_figure.png
image: /images/papers/archon_image.png
featured: true
feature-order: 3
feature-title: "Archon"
feature-description: "Optimizing Inference-Compute Usage"
selected: true
bibtex: |-

  @misc{saadfalcon2024archonarchitecturesearchframework,
      title={Archon: An Architecture Search Framework for Inference-Time Techniques}, 
      author={Jon Saad-Falcon and Adrian Gamarra Lafuente and Shlok Natarajan and Nahum Maru and Hristo Todorov and Etash Guha and E. Kelly Buchanan and Mayee Chen and Neel Guha and Christopher Ré and Azalia Mirhoseini},
      year={2024},
      eprint={2409.15254},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
  }
  
---

Inference-time techniques are emerging as highly effective tools to enhance large language model (LLM) capabilities. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of individual inference-time techniques and the interactions between them. Additionally, efficiently and automatically searching the space of model choices, inference-time techniques, and their compositions is challenging due to the large design space. To address these challenges, we introduce Archon, a modular framework for selecting, combining, and stacking layers of inference-time techniques to construct optimized LLM systems for target benchmarks. Rather than relying on a single LLM called once, we leverage a diverse set of LLMs and inference-time techniques, creating LLM systems greater than the sum of their parts. Archon defines an extensible design space, encompassing techniques such as generation ensembling, repeated sampling, ranking, fusion, critiquing, verification, and unit testing. It transforms the problem of building LLM systems into a hyperparameter optimization objective. Given the available LLMs, inference-time techniques, and compute budget, Archon utilizes hyperparameter search techniques to discover optimized architectures for target benchmark(s). We evaluate Archon architectures across a range of instruction-following, reasoning, and coding benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an average accuracy increase of 15.1 percentage points by using all available LLMs.
